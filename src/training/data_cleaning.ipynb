{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6af9cd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data path: c:\\Users\\hp\\Desktop\\nlp_project\\nlp_project\\data\\01_raw\n",
      "Loading: ..\\..\\data\\01_raw\\ayu-sat-table-a.csv\n",
      "Loading: ..\\..\\data\\01_raw\\ayu-sat-table-b.csv\n",
      "Loading: ..\\..\\data\\01_raw\\ayu-sat-table-c.csv\n",
      "\n",
      "Loaded Rows: A(319), B(514), C(176)\n"
     ]
    }
   ],
   "source": [
    "# file loading\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from IPython.display import display\n",
    "\n",
    "print(f\"Raw data path: {os.path.abspath(raw_data_path)}\")\n",
    "# Fix: Use correct relative path to data folder (go up two levels)\n",
    "raw_data_path = os.path.join('..', '..', 'data', '01_raw')\n",
    "processed_data_path = os.path.join('..', '..', 'data', '02_processed')\n",
    "\n",
    "# ensuring processed directory exists\n",
    "os.makedirs(processed_data_path,exist_ok=True)\n",
    "\n",
    "# These lines were already correct, but verify they are still there\n",
    "file_a=os.path.join(raw_data_path,'ayu-sat-table-a.csv')\n",
    "file_b=os.path.join(raw_data_path,'ayu-sat-table-b.csv')\n",
    "file_c=os.path.join(raw_data_path,'ayu-sat-table-c.csv')\n",
    "\n",
    "column_maps={\n",
    "    'Code':'sat_code',\n",
    "    'Word':'ayurvedic_term',\n",
    "    'Short Defination':'short_definition',\n",
    "    'Long Defination':'long_definition'\n",
    "}\n",
    "\n",
    "def load_and_tag(file_path,tag):\n",
    "    \"\"\"loads a file, renames columns, and adds a source tag.\"\"\"\n",
    "    print(f\"Loading: {file_path}\")\n",
    "    df=pd.read_csv(file_path,encoding='latin1')\n",
    "    df=df.rename(columns={k:v for k,v in column_maps.items() if k in df.columns})\n",
    "\n",
    "    # ensuring all the required columns exists\n",
    "    for col in column_maps.values():\n",
    "        if col not in df.columns:\n",
    "            df[col]=''\n",
    "    \n",
    "    df['source_dataset']=tag\n",
    "    return df[list(column_maps.values())+['source_dataset']]\n",
    "\n",
    "# loading datasets\n",
    "df_a=load_and_tag(file_a,'SAT_A')\n",
    "df_b=load_and_tag(file_b,'SAT_B')\n",
    "df_c=load_and_tag(file_c,'SAT_C')\n",
    "\n",
    "print(f\"\\nLoaded Rows: A({len(df_a)}), B({len(df_b)}), C({len(df_c)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c647fb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows after stacking: 1009\n",
      "Total unique concepts after deduplication: 1009\n"
     ]
    }
   ],
   "source": [
    "# --- 1. MERGE (CONSOLIDATION) ---\n",
    "# Stack the three dataframes vertically\n",
    "master_df = pd.concat([df_a, df_b, df_c], ignore_index=True)\n",
    "print(f\"Total rows after stacking: {len(master_df)}\")\n",
    "\n",
    "# --- 2. STANDARDIZATION ---\n",
    "master_df = master_df.fillna('') # Replace NaNs with empty strings\n",
    "\n",
    "# Apply cleaning: lowercase, strip whitespace, and basic deduplication\n",
    "master_df['sat_code'] = master_df['sat_code'].astype(str).str.strip()\n",
    "master_df['ayurvedic_term_clean'] = master_df['ayurvedic_term'].astype(str).str.lower().str.strip()\n",
    "master_df['short_definition_clean'] = master_df['short_definition'].astype(str).str.lower().str.strip()\n",
    "\n",
    "# Deduplication based on the core unique concept\n",
    "master_df.drop_duplicates(\n",
    "    # Use code and the cleaned term/definition as the unique identifier\n",
    "    subset=['sat_code', 'ayurvedic_term_clean', 'short_definition_clean'], \n",
    "    inplace=True\n",
    ")\n",
    "print(f\"Total unique concepts after deduplication: {len(master_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a4991ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final searchable query terms created: 2017\n",
      "File saved to: ..\\..\\data\\02_processed\\master_concept_map_to_enrich.csv\n",
      "\n",
      "--- Next Step: Manually enrich this file ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sat_code</th>\n",
       "      <th>query_input</th>\n",
       "      <th>source_dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SAT-A</td>\n",
       "      <td>mulabuta-sabdah</td>\n",
       "      <td>SAT_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SAT-A</td>\n",
       "      <td>fundamental terms</td>\n",
       "      <td>SAT_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAT-A.1</td>\n",
       "      <td>science of life</td>\n",
       "      <td>SAT_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAT-A.1</td>\n",
       "      <td>ayurvedah</td>\n",
       "      <td>SAT_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SAT-A.2</td>\n",
       "      <td>ayuh</td>\n",
       "      <td>SAT_A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sat_code        query_input source_dataset\n",
       "0    SAT-A    mulabuta-sabdah          SAT_A\n",
       "0    SAT-A  fundamental terms          SAT_A\n",
       "1  SAT-A.1    science of life          SAT_A\n",
       "1  SAT-A.1          ayurvedah          SAT_A\n",
       "2  SAT-A.2               ayuh          SAT_A"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 3. CREATE SEARCHABLE TERMS LIST ---\n",
    "\n",
    "def create_query_list(row):\n",
    "    \"\"\"Gathers all potential search inputs into a list for a single concept.\"\"\"\n",
    "    # Use a set to automatically handle duplicates and ensure uniqueness\n",
    "    terms = {row['ayurvedic_term_clean']} \n",
    "    \n",
    "    if row['short_definition_clean']:\n",
    "        terms.add(row['short_definition_clean'])\n",
    "        \n",
    "    return list(terms)\n",
    "\n",
    "master_df['searchable_terms'] = master_df.apply(create_query_list, axis=1)\n",
    "\n",
    "# --- 4. EXPLODE THE DATA ---\n",
    "# This is the key step: it creates a new row for every searchable term, \n",
    "# keeping the SAT code mapped to each potential query input.\n",
    "final_mapping_df = master_df.explode('searchable_terms').rename(\n",
    "    columns={'searchable_terms': 'query_input'}\n",
    ")\n",
    "\n",
    "# Final cleanup and selection of columns\n",
    "final_mapping_df = final_mapping_df[['sat_code', 'query_input', 'source_dataset']]\n",
    "final_mapping_df = final_mapping_df[final_mapping_df['query_input'] != '']\n",
    "# Remove duplicates that might have resulted from identical term/definition\n",
    "final_mapping_df.drop_duplicates(subset=['sat_code', 'query_input'], inplace=True)\n",
    "\n",
    "# --- 5. SAVE FOR MANUAL ENRICHMENT ---\n",
    "OUTPUT_FILE = os.path.join(processed_data_path, 'master_concept_map_to_enrich.csv')\n",
    "final_mapping_df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"\\nFinal searchable query terms created: {len(final_mapping_df)}\")\n",
    "print(f\"File saved to: {OUTPUT_FILE}\")\n",
    "print(\"\\n--- Next Step: Manually enrich this file ---\")\n",
    "display(final_mapping_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
